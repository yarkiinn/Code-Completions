{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13128cc1-d286-458e-9c48-7871ad37ce36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\YarkinErdogan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc9eb89-f107-4fb5-aafb-83c5865fffce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17bf4a58-16f6-4d6f-ace4-92f337d7a93f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Create a function to calculate the sum of a se...</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td># Python code\\ndef sum_sequence(sequence):\\n  ...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Generate a Python code for crawling a website ...</td>\n",
       "      <td>website: www.example.com \\ndata to crawl: phon...</td>\n",
       "      <td>import requests\\nimport re\\n\\ndef crawl_websit...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Create a Python list comprehension to get the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[x*x for x in [1, 2, 3, 5, 8, 13]]</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Generate a python script to perform this action.</td>\n",
       "      <td>Given a string, remove all the consecutive dup...</td>\n",
       "      <td>def remove_duplicates(string): \\n    result = ...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Write a python script to generates random numb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>def generate_random_divisible_number():\\n    i...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  Create a function to calculate the sum of a se...   \n",
       "1  Generate a Python code for crawling a website ...   \n",
       "2  Create a Python list comprehension to get the ...   \n",
       "3   Generate a python script to perform this action.   \n",
       "4  Write a python script to generates random numb...   \n",
       "\n",
       "                                               input  \\\n",
       "0                                    [1, 2, 3, 4, 5]   \n",
       "1  website: www.example.com \\ndata to crawl: phon...   \n",
       "2                                                NaN   \n",
       "3  Given a string, remove all the consecutive dup...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                              output  \\\n",
       "0  # Python code\\ndef sum_sequence(sequence):\\n  ...   \n",
       "1  import requests\\nimport re\\n\\ndef crawl_websit...   \n",
       "2                 [x*x for x in [1, 2, 3, 5, 8, 13]]   \n",
       "3  def remove_duplicates(string): \\n    result = ...   \n",
       "4  def generate_random_divisible_number():\\n    i...   \n",
       "\n",
       "                                              prompt  \n",
       "0  Below is an instruction that describes a task....  \n",
       "1  Below is an instruction that describes a task....  \n",
       "2  Below is an instruction that describes a task....  \n",
       "3  Below is an instruction that describes a task....  \n",
       "4  Below is an instruction that describes a task....  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('train_code_completion.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3cf955a0-9326-41f8-aaed-78646150243b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18612, 4)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f863e-d316-451e-b267-da18b148b66c",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d95005ca-c047-4c3d-9b53-8b8ee657738c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_pipeline(data_column):\n",
    "    \"\"\"\n",
    "    Preprocesses the text data in a pandas DataFrame column.\n",
    "    \n",
    "    Args:\n",
    "    data_column (pd.Series): The DataFrame column containing text data.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to hold tokenized sentences\n",
    "    tokenized = []\n",
    "    \n",
    "    # Iterate through each entry in the column\n",
    "    for data in data_column:\n",
    "        # Skip NaN values\n",
    "        if pd.isna(data):\n",
    "            continue\n",
    "        \n",
    "        # Split by newline character\n",
    "        sentences = data.split('\\n')\n",
    "        \n",
    "        # Remove leading and trailing spaces\n",
    "        sentences = [s.strip() for s in sentences]\n",
    "        \n",
    "        # Drop Empty Sentences\n",
    "        sentences = [s for s in sentences if len(s) > 0]\n",
    "        \n",
    "        # Iterate through sentences\n",
    "        for sentence in sentences:\n",
    "            # Convert to lowercase\n",
    "            # sentence = sentence.lower()\n",
    "            \n",
    "            # Convert to a list of words\n",
    "            token = nltk.word_tokenize(sentence)\n",
    "            \n",
    "            # Append to the tokenized list\n",
    "            tokenized.append(token)\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Process the 'output' column in your DataFrame\n",
    "tokenized_sentences = preprocess_pipeline(df['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fd8aa-4116-4c34-8a8a-058417b3639d",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b2104bd-e4e1-4b98-baf8-9931ba7d5f32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(tokenized_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "## Obtain Train and Validation Split \n",
    "train, val = train_test_split(train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c239c3-8715-46a2-861d-00d7f52d204a",
   "metadata": {},
   "source": [
    "### Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "701e3ff4-2115-4e9f-ad41-055d34a1cb63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_the_words(sentences) -> 'dict':\n",
    "    word_counts = {}\n",
    "\n",
    "  # Iterating over sentences\n",
    "    for sentence in sentences:\n",
    "    \n",
    "    # Iterating over Tokens\n",
    "        for token in sentence:\n",
    "    \n",
    "      # Add count for new word\n",
    "            if token not in word_counts.keys():\n",
    "                word_counts[token] = 1\n",
    "        \n",
    "      # Increase count by one\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "        \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20753c-911d-48ef-8625-cc0f0fc2e06e",
   "metadata": {},
   "source": [
    "### Handling Out-of-Vocabulary Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ee2d5dd-8852-4939-8b55-0e555e1e25bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handling_oov(tokenized_sentences, count_threshold) -> 'list':\n",
    "    closed_vocabulary = []\n",
    "\n",
    "  # Obtain frequency dictionary using previously defined function\n",
    "    words_count = count_the_words(tokenized_sentences)\n",
    "    \n",
    "  # Iterate over words and counts \n",
    "    for word, count in words_count.items():\n",
    "    \n",
    "    # Append if it's more(or equal) to the threshold \n",
    "        if count >= count_threshold :\n",
    "            closed_vocabulary.append(word)\n",
    "\n",
    "    return closed_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b40706d-ad81-4c36-bc68-b6757e3702c5",
   "metadata": {},
   "source": [
    "### Tokenization of Unknown Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f159e7da-a4af-4790-b20f-17c35a406950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unk_tokenize(tokenized_sentences, vocabulary, unknown_token = \"<unk>\") -> 'list':\n",
    "    vocabulary = set(vocabulary)\n",
    "\n",
    "  # Create empty list for sentences\n",
    "    new_tokenized_sentences = []\n",
    "  \n",
    "  # Iterate over sentences\n",
    "    for sentence in tokenized_sentences:\n",
    "\n",
    "    # Iterate over sentence and add <unk> \n",
    "    # if the token is absent from the vocabulary\n",
    "        new_sentence = []\n",
    "        for token in sentence:\n",
    "            if token in vocabulary:\n",
    "                new_sentence.append(token)\n",
    "            else:\n",
    "                new_sentence.append(unknown_token)\n",
    "    \n",
    "    # Append sentece to the new list\n",
    "        new_tokenized_sentences.append(new_sentence)\n",
    "\n",
    "    return new_tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdece7a-94dc-4a3c-9a61-84a54ea7fade",
   "metadata": {},
   "source": [
    "### OOV Rate Comparison between Baseline and Main Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2e9ab407-9772-4e23-a970-b8d4003ef697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV rate for your method: 0.375\n",
      "OOV rate for the baseline: 0.4166666666666667\n"
     ]
    }
   ],
   "source": [
    "# Example tokenized sentences and vocabulary\n",
    "def simple_tokenize(text):\n",
    "    return re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "\n",
    "\n",
    "tokenized_sentences = [[\"for\", \"x\", \"in\", \"range(10)\"], [\"def\", \"foo\", \"()\", \":\"]]\n",
    "vocabulary = [\"for\", \"x\", \"in\", \"range\", \"def\", \"foo\", \"10\"]\n",
    "\n",
    "\n",
    "# Apply unk_tokenize\n",
    "modified_sentences = unk_tokenize(tokenized_sentences, vocabulary)\n",
    "\n",
    "# Apply baseline simple tokenization (Whitespace tokenization)\n",
    "baseline_sentences = [simple_tokenize(\" \".join(sentence)) for sentence in tokenized_sentences]\n",
    "\n",
    "\n",
    "# Compare the number of OOV tokens in both approaches\n",
    "def oov_rate(tokenized_sentences, vocabulary):\n",
    "    total_tokens = sum(len(sentence) for sentence in tokenized_sentences)\n",
    "    unk_tokens = sum(1 for sentence in tokenized_sentences for token in sentence if token not in vocabulary)\n",
    "    return unk_tokens / total_tokens\n",
    "\n",
    "print(\"OOV rate for your method:\", oov_rate(modified_sentences, vocabulary))\n",
    "print(\"OOV rate for the baseline:\", oov_rate(baseline_sentences, vocabulary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5128445-ddab-42e4-94bd-524c5c697990",
   "metadata": {},
   "source": [
    "### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33cf63c7-3495-47ce-a2ea-e475b573f443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleansing(train_data, test_data, count_threshold):\n",
    "    vocabulary = handling_oov(train_data, count_threshold)\n",
    "    \n",
    "  # Updated Training Dataset\n",
    "    new_train_data = unk_tokenize(train_data, vocabulary)\n",
    "    \n",
    "  # Updated Test Dataset\n",
    "    new_test_data = unk_tokenize(test_data, vocabulary)\n",
    "\n",
    "    return new_train_data, new_test_data, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "154fc1aa-34e0-495e-a59b-22808c5cbbd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_freq = 6\n",
    "final_train, final_test, vocabulary = cleansing(train, test, min_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf3333-7d77-40aa-9746-a2a7675b67a1",
   "metadata": {},
   "source": [
    "### Count N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "532ac05e-01b2-4b66-bf9c-85d07cca8474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token = \"<s>\", end_token = \"<e>\") -> 'dict':\n",
    "    n_grams = {}\n",
    " \n",
    "  # Iterate over all sentences in the dataset\n",
    "    for sentence in data:\n",
    "        sentence = [start_token]*n + sentence + [end_token]\n",
    "    \n",
    "    # Convert the sentence into a tuple\n",
    "        sentence = tuple(sentence)\n",
    "\n",
    "    # Temp var to store length from start of n-gram to end\n",
    "        m = len(sentence) if n==1 else len(sentence)-1\n",
    "    \n",
    "    # Iterate over this length\n",
    "        for i in range(m):\n",
    "        \n",
    "      # Get the n-gram\n",
    "            n_gram = sentence[i:i+n]\n",
    "    \n",
    "      # Add the count of n-gram as value to our dictionary\n",
    "      # IF n-gram is already present\n",
    "            if n_gram in n_grams.keys():\n",
    "                n_grams[n_gram] += 1\n",
    "      # Add n-gram count\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "        \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c37e21b-25f2-481a-b005-c50a02ae4d60",
   "metadata": {},
   "source": [
    "### Probability for Single Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6cded8d7-4f57-479d-87ca-7f868276ab02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prob_for_single_word(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size, k = 1.0) -> 'float':\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "  # Calculating the count, if exists from our freq dictionary otherwise zero\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "  \n",
    "  # The Denominator\n",
    "    denom = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "  # previous n-gram plus the current word as a tuple\n",
    "    nplus1_gram = previous_n_gram + (word,)\n",
    "\n",
    "  # Calculating the nplus1 count, if exists from our freq dictionary otherwise zero \n",
    "    nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n",
    "\n",
    "  # Numerator\n",
    "    num = nplus1_gram_count + k\n",
    "\n",
    "  # Final Fraction\n",
    "    prob = num / denom\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4717f1f-6087-4741-b10e-6311a30ccc05",
   "metadata": {},
   "source": [
    "### Proability Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56ad300c-e81f-40a9-b537-6c3f9ba89d53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0) -> 'dict':\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "  # Add end and unknown tokens to the vocabulary\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "\n",
    "  # Calculate the size of the vocabulary\n",
    "    vocabulary_size = len(vocabulary)\n",
    "\n",
    "  # Empty dict for probabilites\n",
    "    probabilities = {}\n",
    "\n",
    "  # Iterate over words \n",
    "    for word in vocabulary:\n",
    "    \n",
    "    # Calculate probability\n",
    "        probability = prob_for_single_word(word, previous_n_gram, \n",
    "                                           n_gram_counts, nplus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "    # Create mapping: word -> probability\n",
    "        probabilities[word] = probability\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048c6043-ffe4-4936-b9ea-c360497c639c",
   "metadata": {},
   "source": [
    "### Auto Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eede86af-b9ab-41df-a997-9b90cfd87695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def auto_complete(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "\n",
    "    \n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    # most recent 'n' words\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    # Calculate probabilty for all words\n",
    "    probabilities = probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary, k=k)\n",
    "\n",
    "    # Intialize the suggestion and max probability\n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "\n",
    "    # Iterate over all words and probabilites, returning the max.\n",
    "    # We also add a check if the start_with parameter is provided\n",
    "    for word, prob in probabilities.items():\n",
    "        \n",
    "        if start_with != None: \n",
    "            \n",
    "            if not word.startswith(start_with):\n",
    "                continue \n",
    "\n",
    "        if prob > max_prob: \n",
    "\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "\n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade13f1-718e-4df0-8ee5-335b2ad2cd09",
   "metadata": {},
   "source": [
    "### Get Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b5802ae2-43d7-4226-8002-12d4cf418b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "\n",
    "    # See how many models we have\n",
    "    count = len(n_gram_counts_list)\n",
    "    \n",
    "    # Empty list for suggestions\n",
    "    suggestions = []\n",
    "    \n",
    "    # IMP: Earlier \"-1\"\n",
    "    \n",
    "    # Loop over counts\n",
    "    for i in range(count-1):\n",
    "        \n",
    "        # get n and nplus1 counts\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        nplus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        # get suggestions \n",
    "        suggestion = auto_complete(previous_tokens, n_gram_counts,\n",
    "                                    nplus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        # Append to list\n",
    "        suggestions.append(suggestion)\n",
    "        \n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b04a0771-75db-4fc1-b985-11b7e0b4ff84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    n_model_counts = count_n_grams(final_train, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a478a5-0679-4ff4-a991-8897bbfdba14",
   "metadata": {},
   "source": [
    "### Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "09b020ca-37e1-459f-82eb-6146a3041956",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<unk>', 0.13515289360626737),\n",
       " ('x', 0.3181818181818182),\n",
       " ('for', 0.1111111111111111),\n",
       " ('for', 0.1111111111111111)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = previous_tokens = [\"math\", \".\",\"sqrt\", \"(\"]\n",
    "suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "display(suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a31b83c3-66d8-42d0-8710-25384483b9e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('range', 0.30852360445275046),\n",
       " ('range', 0.3599137931034483),\n",
       " ('range', 0.3874709976798144),\n",
       " ('for', 0.1111111111111111)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = previous_tokens = [\"[\", \"x*x\", \"for\",\"x\",\"in\"]\n",
    "suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "display(suggestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208649b-99b1-4017-a876-fb450a9a5f7e",
   "metadata": {},
   "source": [
    "### Top-1 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1b863885-4487-4ed9-8587-b4f8a1b7aa78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0\n"
     ]
    }
   ],
   "source": [
    "def top_1_accuracy(previous_tokens, n_gram_counts_list, vocabulary, true_token=None):\n",
    "    # Get the top suggestion from the model (k=1)\n",
    "    suggestions = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "    \n",
    "    # Extract the top 1 suggestion\n",
    "    top_1_token = suggestions[0][0]\n",
    "    \n",
    "    # Check if the true token is the top 1 suggestion\n",
    "    if top_1_token == true_token:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Example usage\n",
    "\n",
    "previous_tokens = [\"math\", \".\",\"sqrt\", \"(\"]\n",
    "true_token = \"x\" # the correct next token\n",
    "\n",
    "accuracy_top_1 = top_1_accuracy(previous_tokens, n_gram_counts_list, vocabulary, true_token=true_token)\n",
    "print(f\"Top-1 Accuracy: {accuracy_top_1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca910c-9675-43e1-a7af-b00f4671abca",
   "metadata": {},
   "source": [
    "### Top-k Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "aa32016d-11e2-4548-acca-8eec69ee9253",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-4 Accuracy: 1\n"
     ]
    }
   ],
   "source": [
    "def top_k_accuracy(previous_tokens, n_gram_counts_list, vocabulary, k=1, true_token=None):\n",
    "    # Get the top-k suggestions from the model\n",
    "    suggestions = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "    \n",
    "    # Flatten suggestions to get only the tokens (ignoring the probabilities)\n",
    "    top_k_tokens = [s[0] for s in suggestions][:k]\n",
    "    \n",
    "    # Check if the true token is in the top-k suggestions\n",
    "    if true_token in top_k_tokens:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Example usage\n",
    "previous_tokens = [\"math\", \".\",\"sqrt\", \"(\"]\n",
    "true_token = \"x\" # the correct next token\n",
    "top_k = 4  # specify the value of k\n",
    "\n",
    "# Assuming n_gram_counts_list and vocabulary are defined previously\n",
    "accuracy = top_k_accuracy(previous_tokens, n_gram_counts_list, vocabulary, k=top_k, true_token=true_token)\n",
    "print(f\"Top-{top_k} Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d65e3c-109c-4e1d-aea8-e049c7a1d6e5",
   "metadata": {},
   "source": [
    "### MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "128d52ac-90b9-486a-b54e-0614052cfea3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.5\n"
     ]
    }
   ],
   "source": [
    "def mrr(previous_tokens, n_gram_counts_list, vocabulary, true_token=None):\n",
    "    # Get the top-k suggestions from the model (we assume k=all for MRR)\n",
    "    suggestions = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1000)\n",
    "    \n",
    "    # Find the rank of the true token (itâ€™s the first occurrence in the list)\n",
    "    for rank, (word, _) in enumerate(suggestions, 1):\n",
    "        if word == true_token:\n",
    "            return 1 / rank  # Reciprocal rank\n",
    "\n",
    "    # If the true token is not found, return 0\n",
    "    return 0\n",
    "\n",
    "# Example usage\n",
    "previous_tokens = [\"math\", \".\",\"sqrt\", \"(\"]\n",
    "true_token = \"x\" # the correct next token\n",
    "\n",
    "mrr_value = mrr(previous_tokens, n_gram_counts_list, vocabulary, true_token=true_token)\n",
    "print(f\"MRR: {mrr_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f26b57-34f3-4ef0-b9ee-7d1ff61bc02b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
